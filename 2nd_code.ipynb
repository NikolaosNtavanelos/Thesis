{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140954ed-0bdb-4893-9435-a5e45af6ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "#Training the lcgNetV and lcgNetS with different values of L(layer) to colculate the Normalize Mean Square Error(NMSE)\n",
    "#######################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84121ff-1992-4340-8d00-69d7cef048f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "# Data Generation Function\n",
    "def generate_data(B, K, N, snr_low, snr_high, H_org):\n",
    "    x_ = np.sign(np.random.rand(B, K) - 0.5)\n",
    "    y_ = np.zeros([B, N])\n",
    "    w = np.random.randn(B, N)\n",
    "    Hy_ = x_ * 0\n",
    "    H_ = np.zeros([B, N, K])\n",
    "    HH_ = np.zeros([B, K, K])\n",
    "    SNR_ = np.zeros([B])\n",
    "\n",
    "    for i in range(B):\n",
    "        SNR = np.random.uniform(low=snr_low, high=snr_high)\n",
    "        H = H_org\n",
    "        tmp_snr= (H.T.dot(H)).trace() / K\n",
    "        \n",
    "        \n",
    "        H_[i, :, :] = H\n",
    "        y_[i, :] = H.dot(x_[i, :]) + w[i, :] * np.sqrt(tmp_snr)/np.sqrt(SNR)\n",
    "        Hy_[i, :] = H.T.dot(y_[i, :])\n",
    "        HH_[i, :, :] = H.T.dot(H_[i, :, :])\n",
    "        SNR_[i] = SNR\n",
    "\n",
    "    return y_, H_, Hy_, HH_, x_, SNR_,tmp_snr\n",
    "\n",
    "# NMSE Calculation Function\n",
    "def calculate_nmse(x_true, x_estimated):\n",
    "    mse_per_sample = np.mean((x_true - x_estimated) ** 2, axis=1)\n",
    "    power_per_sample = np.mean(x_true ** 2, axis=1)\n",
    "    nmse_per_sample = mse_per_sample / power_per_sample\n",
    "    avg_nmse = np.mean(nmse_per_sample)\n",
    "    avg_nmse_db = 10 * np.log10(avg_nmse)\n",
    "    return avg_nmse, avg_nmse_db\n",
    "\n",
    "# Validation Function\n",
    "def validate_model(sess, model, y_val, H_val, Hy_val, HH_val, x_val, sigma2_val):\n",
    "    \"\"\"\n",
    "    Validates the model on the validation dataset and returns NMSE.\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        model['y']: y_val,\n",
    "        model['H']: H_val,\n",
    "        model['Hy']: Hy_val,\n",
    "        model['HH']: HH_val,\n",
    "        model['x_true']: x_val,\n",
    "        model['sigma2']: sigma2_val\n",
    "    }\n",
    "    nmse = sess.run(model['nmse'], feed_dict=feed_dict)\n",
    "    return nmse\n",
    "\n",
    "\n",
    "# Model Building Function\n",
    "def build_model(N, K, L, use_vector_params):\n",
    "    y = tf.placeholder(tf.float32, shape=(None, N), name='y')\n",
    "    H = tf.placeholder(tf.float32, shape=(None, N, K), name='H')\n",
    "    Hy = tf.placeholder(tf.float32, shape=(None, K), name='Hy')\n",
    "    HH = tf.placeholder(tf.float32, shape=(None, K, K), name='HH')\n",
    "    x_true = tf.placeholder(tf.float32, shape=(None, K), name='x_true')\n",
    "    sigma2 = tf.placeholder(tf.float32, shape=(), name='noise_variance')  # Noise variance placeholder\n",
    "\n",
    "    I_Nt = tf.eye(K, dtype=tf.float32)\n",
    "\n",
    "    # Calculate A_rm and b_rm\n",
    "    A_rm = HH + sigma2 * I_Nt\n",
    "    b_rm = Hy\n",
    "\n",
    "    # Initialize trainable parameters\n",
    "    if use_vector_params:\n",
    "        alpha = [tf.Variable(tf.zeros([K]), name=f'alpha_{t}') for t in range(L)]\n",
    "        beta = [tf.Variable(tf.zeros([K]), name=f'beta_{t}') for t in range(L)]\n",
    "    else:\n",
    "        alpha = [tf.Variable(tf.zeros([]), name=f'alpha_{t}') for t in range(L)]\n",
    "        beta = [tf.Variable(tf.zeros([]), name=f'beta_{t}') for t in range(L)]\n",
    "\n",
    "    s_hat = tf.zeros_like(x_true, name='s_hat_init')\n",
    "    r = b_rm - tf.squeeze(tf.matmul(A_rm, tf.expand_dims(s_hat, -1)), axis=-1)\n",
    "    d = tf.identity(r)\n",
    "    states = []\n",
    "\n",
    "    for t in range(L):\n",
    "        if use_vector_params:\n",
    "            alpha_t =tf.multiply(alpha[t],d)\n",
    "            beta_t = tf.multiply(beta[t],d)\n",
    "        else:\n",
    "            alpha_t =alpha[t] * d\n",
    "            beta_t = beta[t] * d\n",
    "\n",
    "        s_hat = s_hat + alpha_t\n",
    "        r = r - tf.multiply(alpha[t], tf.squeeze(tf.matmul(A_rm, tf.expand_dims(d, -1)), axis=-1))\n",
    "        d = r + beta_t\n",
    "        states.append(s_hat)\n",
    "\n",
    "    s_hat_final = states[-1]\n",
    "\n",
    "    loss = tf.reduce_mean(tf.square(s_hat_final - x_true))\n",
    "    nmse = tf.reduce_mean(tf.square(s_hat_final - x_true)) / tf.reduce_mean(tf.square(x_true))\n",
    "\n",
    "    trinit = 0.001  # Initial learning rate\n",
    "    train_ops = [tf.train.AdamOptimizer(trinit).minimize(loss, var_list=[alpha[t], beta[t]]) for t in range(L)]\n",
    "\n",
    "    fine_tune_ops = {\n",
    "    lr: tf.train.AdamOptimizer(lr).minimize(loss, var_list=[\n",
    "        var for i in range(t) for var in [alpha[i], beta[i]]\n",
    "    ])\n",
    "    for lr in [0.001, 0.0005, 0.0001, 0.00005]\n",
    "}\n",
    "\n",
    "\n",
    "    return {\n",
    "        'train_ops': train_ops,\n",
    "        'fine_tune_ops': fine_tune_ops,\n",
    "        'loss': loss,\n",
    "        'nmse': nmse,\n",
    "        'y': y,\n",
    "        'H': H,\n",
    "        'Hy': Hy,\n",
    "        'HH': HH,\n",
    "        'x_true': x_true,\n",
    "        'sigma2': sigma2,\n",
    "        's_hat_final': s_hat_final\n",
    "    }\n",
    "\n",
    "# Training with Fine-Tuning\n",
    "def train_for_snr(sess, snr, model, patience=3):\n",
    "    \"\"\"\n",
    "    Layer-wise training and fine-tuning for a specific SNR value.\n",
    "    \"\"\"\n",
    "    print(f\"Training and Fine-tuning for SNR = {snr[0]} dB...\")\n",
    "    L = len(model['train_ops'])\n",
    "    for t in range(L):\n",
    "        print(f\"Training Layer {t+1}/{L} for SNR = {snr[0]} dB...\")\n",
    "\n",
    "        y_train, H_train, Hy_train, HH_train, x_train, _ ,tmp_snr= generate_data(B, K, N, snr[0], snr[1], H_org)\n",
    "        y_val, H_val, Hy_val, HH_val, x_val, _,tmp_snr = generate_data(B, K, N, snr[0], snr[1], H_org)\n",
    "\n",
    "        sigma2_val = tmp_snr/ snr[0]\n",
    "        best_val_nmse = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(1000):\n",
    "            feed_dict = {\n",
    "                model['y']: y_train,\n",
    "                model['H']: H_train,\n",
    "                model['Hy']: Hy_train,\n",
    "                model['HH']: HH_train,\n",
    "                model['x_true']: x_train,\n",
    "                model['sigma2']: sigma2_val\n",
    "            }\n",
    "            sess.run(model['train_ops'][t], feed_dict=feed_dict)\n",
    "\n",
    "            val_nmse = validate_model(sess, model, y_val, H_val, Hy_val, HH_val, x_val, sigma2_val)\n",
    "            print(f\"Layer {t+1}, Epoch {epoch+1}: Validation NMSE = {val_nmse:.6f}\")\n",
    "\n",
    "            if val_nmse < best_val_nmse:\n",
    "                best_val_nmse = val_nmse\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at Layer {t+1}, Epoch {epoch+1} (Best Validation NMSE = {best_val_nmse:.6f})\")\n",
    "                break\n",
    "\n",
    "        print(f\"Fine-tuning Layers 1 to {t+1} for SNR = {snr[0]} dB...\")\n",
    "        for lr, fine_tune_op in model['fine_tune_ops'].items():\n",
    "            for epoch in range(200):\n",
    "                feed_dict = {\n",
    "                    model['y']: y_train,\n",
    "                    model['H']: H_train,\n",
    "                    model['Hy']: Hy_train,\n",
    "                    model['HH']: HH_train,\n",
    "                    model['x_true']: x_train,\n",
    "                    model['sigma2']: sigma2_val\n",
    "                }\n",
    "                sess.run(fine_tune_op, feed_dict=feed_dict)\n",
    "                val_nmse = validate_model(sess, model, y_val, H_val, Hy_val, HH_val, x_val, sigma2_val)\n",
    "                print(f\"Fine-tuning LR={lr}, Epoch {epoch+1}: Validation NMSE = {val_nmse:.6f}\")\n",
    " \n",
    "# Function to Save Parameters to File\n",
    "def save_params_to_file(params, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(params, f)\n",
    "\n",
    "# Function to Load Parameters from File\n",
    "def load_params_from_file(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Parameters\n",
    "N, K = 12, 12\n",
    "B = 1000  # Reduced batch size for memory efficiency\n",
    "#H_org = np.random.randn(N, K)\n",
    "#frobenius_norm = np.linalg.norm(H_org, 'fro')  # Frobenius norm of the matrix\n",
    "#H_org= H_org/ frobenius_norm\n",
    "snr_values = [10**(20/10.0), 10**(20/10.0)]\n",
    "L_list = [2,4,6,8,10]  # Number of layers to test\n",
    "\n",
    "# File Paths for Parameters\n",
    "scalar_params_file = \"scalar_params.pkl\"\n",
    "vector_params_file = \"vector_params.pkl\"\n",
    "\n",
    "# Train Scalar and Vector Models with Fine-Tuning\n",
    "for L in L_list:\n",
    "    print(f\"\\nTraining Scalar Model with L={L}\")\n",
    "    tf.reset_default_graph()\n",
    "    scalar_model = build_model(N, K, L, use_vector_params=False)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_for_snr(sess, snr_values, scalar_model)\n",
    "        \n",
    "        # Save Scalar Model Parameters\n",
    "        scalar_params = {var.name: sess.run(var) for var in tf.global_variables()}\n",
    "        save_params_to_file(scalar_params, f\"scalar_params_L{L}.pkl\")\n",
    "        print(f\"Scalar Model Parameters for L={L} saved to scalar_params_L{L}.pkl\")\n",
    "\n",
    "    print(f\"\\nTraining Vector Model with L={L}\")\n",
    "    tf.reset_default_graph()\n",
    "    vector_model = build_model(N, K, L, use_vector_params=True)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        train_for_snr(sess, snr_values, vector_model)\n",
    "        \n",
    "        # Save Vector Model Parameters\n",
    "        vector_params = {var.name: sess.run(var) for var in tf.global_variables()}\n",
    "        save_params_to_file(vector_params, f\"vector_params_L{L}.pkl\")\n",
    "        print(f\"Vector Model Parameters for L={L} saved to vector_params_L{L}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f2f19-ebc3-414d-bcb9-f5c5d71553eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################\n",
    "# The testing part where compare  the LcgNetV,LcgNetS and LMMSE base to NMSE \n",
    "########################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b57ced-6483-43ef-aee9-4460df1ae88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmse_results = {'scalar': [], 'vector': []}\n",
    "lmmse_results = []  # To store LMMSE NMSE for each L\n",
    "iterations = 1  # Number of runs to average NMSE\n",
    "L_list=[2,4,6,8,10]\n",
    "# Initialize accumulators for averaging\n",
    "scalar_nmse_accum = {L: [] for L in L_list}\n",
    "vector_nmse_accum = {L: [] for L in L_list}\n",
    "lmmse_nmse_accum = {L: [] for L in L_list}\n",
    "\n",
    "for i in range(iterations):\n",
    "    print(f\"\\nIteration {i+1}/{iterations}\")\n",
    "\n",
    "    # Generate test data once per iteration\n",
    "    y_test, H_test, Hy_test, HH_test, x_test, _,tmp_snr_= generate_data(1000, K, N, snr_low=snr_values[0], snr_high=snr_values[0], H_org=H_org)\n",
    "\n",
    "    for L in L_list:\n",
    "        print(f\"\\nTesting Models with L={L}\")\n",
    "\n",
    "        # Compute LMMSE Estimate and NMSE in TensorFlow\n",
    "        tf.reset_default_graph()\n",
    "        y = tf.placeholder(tf.float32, shape=(None, N), name='y')\n",
    "        H = tf.placeholder(tf.float32, shape=(None, N, K), name='H')\n",
    "        x_true = tf.placeholder(tf.float32, shape=(None, K), name='x_true')\n",
    "        sigma2 = tf.placeholder(tf.float32, name='sigma2')\n",
    "\n",
    "        # LMMSE Calculation\n",
    "        #tmp_snr = (H.T.dot(H)).trace() / K\n",
    "        H_t = tf.transpose(H, perm=[0, 2, 1])  # Transpose of H\n",
    "     \n",
    "        I = tf.eye(K, batch_shape=[tf.shape(H)[0]])  # Identity matrix\n",
    "        HTH = tf.matmul(H_t, H)  # H^T * H\n",
    "        inv = tf.linalg.inv(HTH + sigma2 * I)  # Inverse (H^T * H + sigma^2 * I)\n",
    "        HTy = tf.matmul(H_t, tf.expand_dims(y, -1))  # H^T * y\n",
    "        x_lmmse = tf.squeeze(tf.matmul(inv, HTy), axis=-1)  # LMMSE estimate\n",
    "       \n",
    "        # LMMSE NMSE\n",
    "        nmse_lmmse = tf.reduce_mean(tf.square(x_lmmse - x_true)) / tf.reduce_mean(tf.square(x_true))\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            # Feed test data to compute LMMSE NMSE\n",
    "            feed_dict = {\n",
    "                y: y_test,\n",
    "                H: H_test,\n",
    "                x_true: x_test,\n",
    "                sigma2: tmp_snr_/snr_values[0]\n",
    "\n",
    "            }\n",
    "            nmse_lmmse_value = sess.run(nmse_lmmse, feed_dict=feed_dict)\n",
    "            lmmse_nmse_accum[L].append(nmse_lmmse_value)  # Accumulate NMSE for LMMSE\n",
    "\n",
    "        # Scalar Model Testing\n",
    "        tf.reset_default_graph()\n",
    "        scalar_model = build_model(N, K, L, use_vector_params=False)\n",
    "        scalar_params_file = f\"scalar_params_L{L}.pkl\"\n",
    "        scalar_params = load_params_from_file(scalar_params_file)  # Load scalar model parameters\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Restore parameters for scalar model\n",
    "            for var in tf.global_variables():\n",
    "                if var.name in scalar_params:\n",
    "                    sess.run(var.assign(scalar_params[var.name]))\n",
    "\n",
    "            # Scalar model feed dictionary\n",
    "            feed_dict = {\n",
    "                scalar_model['y']: y_test,\n",
    "                scalar_model['H']: H_test,\n",
    "                scalar_model['Hy']: Hy_test,\n",
    "                scalar_model['HH']: HH_test, \n",
    "                scalar_model['x_true']: x_test,\n",
    "                scalar_model['sigma2']: tmp_snr_/ snr_values[0]\n",
    "            }\n",
    "\n",
    "            # Compute NMSE for scalar model\n",
    "            nmse_scalar = sess.run(scalar_model['nmse'], feed_dict=feed_dict)\n",
    "            scalar_nmse_accum[L].append(nmse_scalar)\n",
    "\n",
    "        # Vector Model Testing\n",
    "        tf.reset_default_graph()\n",
    "        vector_model = build_model(N, K, L, use_vector_params=True)\n",
    "        vector_params_file = f\"vector_params_L{L}.pkl\"\n",
    "        vector_params = load_params_from_file(vector_params_file)  # Load vector model parameters\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # Restore parameters for vector model\n",
    "            for var in tf.global_variables():\n",
    "                if var.name in vector_params:\n",
    "                    sess.run(var.assign(vector_params[var.name]))\n",
    "\n",
    "            # Vector model feed dictionary\n",
    "            feed_dict = {\n",
    "                vector_model['y']: y_test,\n",
    "                vector_model['H']: H_test,\n",
    "                vector_model['Hy']: Hy_test,\n",
    "                vector_model['HH']: HH_test,\n",
    "                vector_model['x_true']: x_test,\n",
    "                vector_model['sigma2']: tmp_snr_/ snr_values[0]\n",
    "            }\n",
    "\n",
    "            # Compute NMSE for vector model\n",
    "            nmse_vector = sess.run(vector_model['nmse'], feed_dict=feed_dict)\n",
    "            vector_nmse_accum[L].append(nmse_vector)\n",
    "\n",
    "# Compute Average NMSE and Convert to dB\n",
    "nmse_results_db = {'scalar': [], 'vector': []}\n",
    "lmmse_results_db = []\n",
    "\n",
    "for L in L_list:\n",
    "    scalar_avg_nmse = np.mean(scalar_nmse_accum[L])\n",
    "    vector_avg_nmse = np.mean(vector_nmse_accum[L])\n",
    "    lmmse_avg_nmse = np.mean(lmmse_nmse_accum[L])\n",
    "\n",
    "    nmse_results_db['scalar'].append((L, 10 * np.log10(scalar_avg_nmse)))\n",
    "    nmse_results_db['vector'].append((L, 10 * np.log10(vector_avg_nmse)))\n",
    "    lmmse_results_db.append((L, 10 * np.log10(lmmse_avg_nmse)))\n",
    "\n",
    "print(\"\\nNMSE Results (dB):\")\n",
    "print(\"Scalar Model:\", nmse_results_db['scalar'])\n",
    "print(\"Vector Model:\", nmse_results_db['vector'])\n",
    "print(\"LMMSE (dB):\", lmmse_results_db)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
