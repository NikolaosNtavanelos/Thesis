{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d323c0b-9b4d-4495-8fb5-1d2cda4b986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ###########################################################################################################################\n",
    "#Testing the clasicall detectors , fully_con and detnet under the same data for BPSK modulation with 10x12 fixed channel\n",
    "###########################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edb68948-2f39-4d24-94c8-7d4951d5da67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DetNet Training Step 0, Loss: 217.48635864257812\n",
      "DetNet Training Step 500, Loss: 108.69695281982422\n",
      "DetNet Training Step 1000, Loss: 76.4346923828125\n",
      "DetNet Training Step 1500, Loss: 58.873138427734375\n",
      "DetNet Training Step 2000, Loss: 48.48405456542969\n",
      "DetNet Training Step 2500, Loss: 39.982452392578125\n",
      "DetNet Training Step 3000, Loss: 34.21874237060547\n",
      "DetNet Training Step 3500, Loss: 28.26943588256836\n",
      "DetNet Training Step 4000, Loss: 23.192642211914062\n",
      "DetNet Training Step 4500, Loss: 20.69095802307129\n",
      "DetNet Training Step 5000, Loss: 19.244014739990234\n",
      "DetNet Training Step 5500, Loss: 17.37433624267578\n",
      "DetNet Training Step 6000, Loss: 15.781689643859863\n",
      "DetNet Training Step 6500, Loss: 14.876791000366211\n",
      "DetNet Training Step 7000, Loss: 14.21640396118164\n",
      "DetNet Training Step 7500, Loss: 13.70832633972168\n",
      "DetNet Training Step 8000, Loss: 12.419853210449219\n",
      "DetNet Training Step 8500, Loss: 11.797231674194336\n",
      "DetNet Training Step 9000, Loss: 11.271268844604492\n",
      "DetNet Training Step 9500, Loss: 11.20083999633789\n",
      "FC-NN Training Step 0, BER: 1.0\n",
      "FC-NN Training Step 500, BER: 0.08841666666666667\n",
      "FC-NN Training Step 1000, BER: 0.01525\n",
      "FC-NN Training Step 1500, BER: 0.013\n",
      "FC-NN Training Step 2000, BER: 0.011083333333333334\n",
      "FC-NN Training Step 2500, BER: 0.011333333333333334\n",
      "FC-NN Training Step 3000, BER: 0.007833333333333333\n",
      "FC-NN Training Step 3500, BER: 0.006\n",
      "FC-NN Training Step 4000, BER: 0.0069166666666666664\n",
      "FC-NN Training Step 4500, BER: 0.008166666666666666\n",
      "FC-NN Training Step 5000, BER: 0.00425\n",
      "FC-NN Training Step 5500, BER: 0.009083333333333334\n",
      "FC-NN Training Step 6000, BER: 0.006166666666666667\n",
      "FC-NN Training Step 6500, BER: 0.005666666666666667\n",
      "FC-NN Training Step 7000, BER: 0.00475\n",
      "FC-NN Training Step 7500, BER: 0.005416666666666667\n",
      "FC-NN Training Step 8000, BER: 0.004583333333333333\n",
      "FC-NN Training Step 8500, BER: 0.007166666666666667\n",
      "FC-NN Training Step 9000, BER: 0.00425\n",
      "FC-NN Training Step 9500, BER: 0.005333333333333333\n",
      "Testing at SNR 1.2589254117941673\n",
      "Testing at SNR 2.046968271807521\n",
      "Testing at SNR 3.328298139454621\n",
      "Testing at SNR 5.411695265464638\n",
      "Testing at SNR 8.79922543569107\n",
      "Testing at SNR 14.307229891937572\n",
      "Testing at SNR 23.263050671536263\n",
      "Testing at SNR 37.82489906389386\n",
      "Testing at SNR 61.501950427522104\n",
      "Testing at SNR 100.0\n",
      "BER Results for DetNet: [[4.65145000e-01 3.67106667e-01 2.67702500e-01 1.70077500e-01\n",
      "  8.94491667e-02 3.63083333e-02 1.07291667e-02 2.44000000e-03\n",
      "  2.60000000e-04 1.50000000e-05]]\n",
      "BER Results for FC-NN: [[3.15678333e-01 2.61482500e-01 2.04330000e-01 1.44229167e-01\n",
      "  8.85941667e-02 4.50900000e-02 1.84791667e-02 5.70833333e-03\n",
      "  1.35416667e-03 2.16666667e-04]]\n",
      "BER Results for Zero Forcing: [[0.38484833 0.35530333 0.32238083 0.282555   0.23927167 0.1949175\n",
      "  0.14788917 0.1046675  0.067265   0.04031833]]\n",
      "BER Results for MMSE: [[0.20317083 0.1648575  0.12869167 0.09511333 0.06674583 0.04497833\n",
      "  0.0283775  0.01777833 0.01158667 0.00764583]]\n",
      "BER Results for Conjugate Gradient: [[0.38484833 0.35530333 0.32238083 0.282555   0.2392725  0.1949175\n",
      "  0.14788917 0.1046675  0.067265   0.04031833]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "# Data generation and other helper functions (generate_data, zero_forcing_detector, mmse_detector, conjugate_gradient_correct) remain the same\n",
    "\n",
    "# DetNet Model definition (remains unchanged)\n",
    "def DetNetModel(K, L, v_size, hl_size, res_alpha, LOG_LOSS, startingLearningRate, decay_step_size, decay_factor):\n",
    "    \"\"\"Define and train the DetNet model.\"\"\"\n",
    "    HY = tf.compat.v1.placeholder(tf.float32, shape=[None, K], name='HY')\n",
    "    X = tf.compat.v1.placeholder(tf.float32, shape=[None, K], name='X')\n",
    "    HH = tf.compat.v1.placeholder(tf.float32, shape=[None, K, K], name='HH')\n",
    "    X_IND = tf.compat.v1.placeholder(tf.float32, shape=[None, K, 2], name='X_IND')\n",
    "    batch_size = tf.shape(HY)[0]\n",
    "    S2=[]\n",
    "    S2.append(tf.zeros([batch_size,2*K]))\n",
    "    S1=[]\n",
    "    S1.append(tf.zeros([batch_size,K]))\n",
    "    S_NO_res=[]\n",
    "    S_NO_res.append(tf.zeros([batch_size,K]))\n",
    "    V=[]\n",
    "    V.append(tf.zeros([batch_size,v_size]))\n",
    "    LOSS=[]\n",
    "    LOSS.append(tf.zeros([]))\n",
    "    BER=[] \n",
    "    BER.append(tf.zeros([]))\n",
    "    delta = tf.Variable(tf.zeros(L * 2), name='delta')  # Fixed shape\n",
    "    Z1 = []\n",
    "    ZZ1 = []\n",
    "    W11 = []\n",
    "    w11 = []\n",
    "    W22 = []\n",
    "    w22 = [] \n",
    "    W33 = []\n",
    "    w33 = []\n",
    "\n",
    "    for i in range(1, L):\n",
    "        temp1 = tf.matmul(tf.expand_dims(S1[-1], 1), HH)\n",
    "        temp1 = tf.squeeze(temp1, 1)\n",
    "\n",
    "        Z1 = S1[-1] - delta[(i - 1) * 2] * HY + delta[(i - 1) * 2 + 1] * temp1\n",
    "        Z = tf.concat([Z1, V[-1]], 1)\n",
    "\n",
    "        ZZ, Wtemp, wtemp = relu_layer(Z, K + v_size, hl_size, 'relu' + str(i))\n",
    "        W11.append(Wtemp)\n",
    "        w11.append(wtemp)\n",
    "        ZZ1.append((ZZ))\n",
    "\n",
    "        S2_temp,W22_temp,w22_temp = sign_layer(ZZ, hl_size, 2*K, 'sign' + str(i))\n",
    "        S2.append(S2_temp)\n",
    "        W22.append(W22_temp)\n",
    "        w22.append(w22_temp)\n",
    "        \n",
    "\n",
    "        S_NO_res.append(S2[i])\n",
    "        S2[i]=(1-res_alpha)*S2[i]+res_alpha*S2[i-1]\n",
    "        V_temp ,  W33_temp , w33_temp=affine_layer(ZZ , hl_size , v_size,'aff'+str(i))\n",
    "        V.append(V_temp)\n",
    "        W33.append(W33_temp)\n",
    "        w33.append(w33_temp)\n",
    "\n",
    "        V[i]=(1-res_alpha)*V[i]+res_alpha*V[i-1]  \n",
    "    \n",
    "        S3 = tf.reshape(S2[i],[batch_size,K,2])\n",
    "    \n",
    "        temp_0 = S3[:,:,0]\n",
    "        temp_1 = S3[:,:,1]\n",
    "    \n",
    "        temp_2 = 1*temp_0 + (-1)*temp_1\n",
    "        S1.append(temp_2)\n",
    "\n",
    "        X_IND_reshaped = tf.reshape(X_IND, [batch_size, 2 * K])\n",
    "        if LOG_LOSS == 1:\n",
    "            LOSS.append(tf.reduce_mean(tf.square(X_IND_reshaped - S2[-1])) * tf.cast(i, tf.float32))\n",
    "        else:\n",
    "            LOSS.append(tf.reduce_mean(tf.square(X_IND_reshaped - S2[-1])))\n",
    "\n",
    "        BER.append(tf.reduce_mean(tf.cast(tf.not_equal(X_IND, tf.round(S3)), tf.float32)))\n",
    "\n",
    "    TOTAL_LOSS = tf.add_n(LOSS)\n",
    "\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(startingLearningRate, global_step, decay_step_size, decay_factor, staircase=True)\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(TOTAL_LOSS)\n",
    "\n",
    "    init_op = tf.compat.v1.global_variables_initializer()\n",
    "    return saver, train_step, init_op, HY, HH, X, X_IND, batch_size, S3, TOTAL_LOSS, BER\n",
    "\n",
    "# Neural Network Fully Connected model definition\n",
    "def build_fc_nn(NNinput, batchSize, N, K, fc_size):\n",
    "    W_fc1 = tf.Variable(tf.truncated_normal([N, fc_size], stddev=0.05))\n",
    "    b_fc1 = tf.Variable(tf.constant(0.1, shape=[fc_size]))\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(NNinput, W_fc1) + b_fc1)\n",
    "\n",
    "    W_fc2 = tf.Variable(tf.truncated_normal([fc_size, fc_size], stddev=0.05))\n",
    "    b_fc2 = tf.Variable(tf.constant(0.1, shape=[fc_size]))\n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "    W_fc3 = tf.Variable(tf.truncated_normal([fc_size, fc_size], stddev=0.05))\n",
    "    b_fc3 = tf.Variable(tf.constant(0.1, shape=[fc_size]))\n",
    "    h_fc3 = tf.nn.relu(tf.matmul(h_fc2, W_fc3) + b_fc3)\n",
    "\n",
    "    W_fc4 = tf.Variable(tf.truncated_normal([fc_size, fc_size], stddev=0.05))\n",
    "    b_fc4 = tf.Variable(tf.constant(0.1, shape=[fc_size]))\n",
    "    h_fc4 = tf.nn.relu(tf.matmul(h_fc3, W_fc4) + b_fc4)\n",
    "\n",
    "    W_fc5 = tf.Variable(tf.truncated_normal([fc_size, fc_size], stddev=0.05))\n",
    "    b_fc5 = tf.Variable(tf.constant(0.1, shape=[fc_size]))\n",
    "    h_fc5 = tf.nn.relu(tf.matmul(h_fc4, W_fc5) + b_fc5)\n",
    "\n",
    "    # Output layer\n",
    "    W_fc6 = tf.Variable(tf.truncated_normal([fc_size, 2*K], stddev=0.05))\n",
    "    b_fc6 =tf.Variable(tf.constant(0.1, shape=[2 * K]))\n",
    "    h_fc6 = tf.matmul(h_fc5, W_fc6) + b_fc6\n",
    "\n",
    "    output = tf.reshape(h_fc6, [batchSize, K, 2])\n",
    "\n",
    "    return output\n",
    "def test_detnet_and_fc_nn(snrdb_low_test, snrdb_high_test, H, N, K, B, test_iter, train_iter, num_snr):\n",
    "    fc_size = 300\n",
    "    L = 30 # Increase number of layers in DetNet\n",
    "    v_size = 4 * K  # Dimension of the auxiliary variable in DetNet\n",
    "    hl_size = 4 * K  # Hidden layer size for DetNet\n",
    "    res_alpha = 0.9 # Adjust residual scaling\n",
    "    LOG_LOSS = 1\n",
    "    startingLearningRate = 0.0005  # Reduce the learning rate\n",
    "    decay_step_size = 1000\n",
    "    decay_factor = 0.95  # More gradual learning rate decay\n",
    "    detnet_train_batch_size = B\n",
    "\n",
    "    # Build DetNet model\n",
    "    saver_detnet, train_step_detnet, init_op_detnet, HY_detnet, HH_detnet, X_detnet, X_IND_detnet, batch_size_detnet, S3_detnet,loss_detnet, ber_detnet = DetNetModel(K, L, v_size, hl_size, res_alpha, LOG_LOSS, startingLearningRate, decay_step_size, decay_factor)\n",
    "\n",
    "    # Build FC-NN model\n",
    "    NNinput = tf.compat.v1.placeholder(tf.float32, shape=[None, N])\n",
    "    org_signal = tf.compat.v1.placeholder(tf.float32, shape=[None, K])\n",
    "    X_IND = tf.compat.v1.placeholder(tf.float32, shape=[None, K, 2])\n",
    "    batchSize = tf.compat.v1.placeholder(tf.int32)\n",
    "\n",
    "    output_fc_nn = build_fc_nn(NNinput, batchSize, N, K, fc_size)\n",
    "    ssd_fc_nn = tf.reduce_sum(tf.reduce_sum(tf.square(X_IND - output_fc_nn)))\n",
    "    temp_0 = output_fc_nn[:, :, 0]\n",
    "    temp_1 = output_fc_nn[:, :, 1]\n",
    "    output2_fc_nn = 1 * temp_0 + (-1) * temp_1\n",
    "    rounded_fc_nn = tf.sign(output2_fc_nn)\n",
    "    eq_fc_nn = tf.equal(rounded_fc_nn, org_signal)\n",
    "    eq2_fc_nn = tf.reduce_sum(tf.cast(eq_fc_nn, tf.int32))\n",
    "    train_step_fc_nn = tf.train.AdamOptimizer(0.0003).minimize(ssd_fc_nn)\n",
    "\n",
    "    # Use a single session for both models\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables\n",
    "        sess.run(init_op_detnet)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Training DetNet model\n",
    "        for i in range(train_iter):\n",
    "            batch_Y, batch_H, batch_HY, batch_HH, batch_X, SNR1, x_ind,tmp_snr = generate_data(B, K, N, snrdb_low_test, snrdb_high_test , H)\n",
    "            sess.run(train_step_detnet, feed_dict={HY_detnet: batch_HY, HH_detnet: batch_HH, X_detnet: batch_X, X_IND_detnet: x_ind})\n",
    "            if i % 500== 0:\n",
    "                loss_val = sess.run(loss_detnet, feed_dict={HY_detnet: batch_HY, HH_detnet: batch_HH, X_detnet: batch_X, X_IND_detnet: x_ind})\n",
    "                print(f'DetNet Training Step {i}, Loss: {loss_val}')\n",
    "\n",
    "        # Training FC-NN model\n",
    "        for i in range(train_iter):\n",
    "            batch_Y, batch_H, batch_HY, batch_HH, batch_X, SNR1, x_ind,tmp_snr = generate_data(B, K, N, snrdb_low_test , snrdb_high_test , H)\n",
    "            sess.run(train_step_fc_nn, feed_dict={NNinput: batch_Y, org_signal: batch_X, batchSize: B, X_IND: x_ind})\n",
    "            if i % 500== 0:\n",
    "                eq2_val = sess.run(eq2_fc_nn, feed_dict={NNinput: batch_Y, org_signal: batch_X, batchSize: B, X_IND: x_ind})\n",
    "                batch_ber = (K * B - eq2_val) / (K * B)\n",
    "                print(f'FC-NN Training Step {i}, BER: {batch_ber}')\n",
    "\n",
    "        # Testing phase\n",
    "        bers_detnet = np.zeros((1, num_snr))\n",
    "        bers_fc_nn = np.zeros((1, num_snr))\n",
    "        bers_zf = np.zeros((1, num_snr))\n",
    "        bers_mmse = np.zeros((1, num_snr))\n",
    "        bers_cg = np.zeros((1, num_snr))\n",
    "\n",
    "        snrdb_list = np.linspace(1.0, 20.0, num_snr)\n",
    "        snr_list = 10.0 ** (snrdb_list / 10.0)\n",
    "\n",
    "        for i_snr in range(num_snr):\n",
    "            Cur_SNR=snr_list[i_snr]\n",
    "            print(f\"Testing at SNR {Cur_SNR}\")\n",
    "            for i in range(test_iter):\n",
    "                batch_Y, batch_H, batch_HY, batch_HH, batch_X, SNR1, x_ind,tmp_snr= generate_data(1000, K, N, Cur_SNR, Cur_SNR, H)\n",
    "\n",
    "                # DetNet Testing\n",
    "                S3_val = sess.run(S3_detnet, feed_dict={HY_detnet: batch_HY, HH_detnet: batch_HH, X_detnet: batch_X, X_IND_detnet: x_ind})\n",
    "                incorrect_bits_detnet = np.sum(np.not_equal(np.sign(S3_val), x_ind))\n",
    "                batch_ber_detnet = incorrect_bits_detnet / (K * 1000)\n",
    "                bers_detnet[0][i_snr] += batch_ber_detnet / test_iter\n",
    "\n",
    "                # FC-NN Testing\n",
    "                eq2_val = sess.run(eq2_fc_nn, feed_dict={NNinput: batch_Y, org_signal: batch_X, batchSize: 1000, X_IND: x_ind})\n",
    "                incorrect_bits_fc_nn = K * 1000 - eq2_val\n",
    "                batch_ber_fc_nn = incorrect_bits_fc_nn / (K * 1000)\n",
    "                bers_fc_nn[0][i_snr] += batch_ber_fc_nn / test_iter\n",
    "\n",
    "                # Zero Forcing Detection\n",
    "                x_hat_zf = np.zeros((1000, K))\n",
    "                for b in range(1000):\n",
    "                    x_hat_zf[b, :] = zero_forcing_detector(batch_H[b, :, :], batch_Y[b, :])\n",
    "                rounded_zf = np.sign(x_hat_zf)\n",
    "                incorrect_bits_zf = np.sum(np.not_equal(rounded_zf, batch_X))\n",
    "                batch_ber_zf=incorrect_bits_zf/(K * 1000)\n",
    "                bers_zf[0][i_snr] += batch_ber_zf /test_iter\n",
    "\n",
    "                # MMSE Detection\n",
    "                x_hat_mmse = np.zeros((1000, K))\n",
    "                for b in range(1000):\n",
    "                    x_hat_mmse[b, :] = mmse_detector(batch_H[b, :, :], batch_Y[b, :], tmp_snr/ Cur_SNR)\n",
    "                rounded_mmse = np.sign(x_hat_mmse)\n",
    "                incorrect_bits_mmse = np.sum(np.not_equal(rounded_mmse, batch_X))\n",
    "                batch_ber_mmse=incorrect_bits_mmse/(K*1000)\n",
    "                bers_mmse[0][i_snr] += batch_ber_mmse/ test_iter\n",
    "                \n",
    "                # Conjugate Gradient Detection\n",
    "                x_hat_cg = np.zeros((1000, K))\n",
    "                s_init = np.zeros(K)\n",
    "                for b in range(1000):\n",
    "                    x_hat_cg[b, :] = conjugate_gradient_correct(batch_HH[b, :, :], batch_HY[b, :], s_init, max_iterations=1000, tolerance=1e-6)[0][:, -1]\n",
    "                rounded_cg = np.sign(x_hat_cg)\n",
    "                incorrect_bits_cg = np.sum(np.not_equal(rounded_cg, batch_X))\n",
    "                bers_cg[0][i_snr] += incorrect_bits_cg / (K * 1000* test_iter)\n",
    "\n",
    "        print(f\"BER Results for DetNet: {bers_detnet}\")\n",
    "        print(f\"BER Results for FC-NN: {bers_fc_nn}\")\n",
    "        print(f\"BER Results for Zero Forcing: {bers_zf}\")\n",
    "        print(f\"BER Results for MMSE: {bers_mmse}\")\n",
    "        print(f\"BER Results for Conjugate Gradient: {bers_cg}\")\n",
    "\n",
    "# Test both models and detectors\n",
    "snr_low_= 1.0\n",
    "snr_high_= 20.0\n",
    "snr_low=20**(snr_low_/10.0)\n",
    "snr_high=20**(snr_high_/10.0)\n",
    "N = 12\n",
    "K = 12\n",
    "B =1000\n",
    "test_iter = 100\n",
    "train_iter = 10000\n",
    "num_snr = 10\n",
    "H = np.random.randn(N, K) \n",
    "\n",
    "\n",
    "test_detnet_and_fc_nn(snr_low, snr_high, H, N, K, B, test_iter, train_iter, num_snr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32854c7-bf39-4a27-b398-5138926a0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "#help functions for the main code \n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b97a7e-06bd-4423-8f82-470afbdce3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(B,K,N,snr_low,snr_high,H_org):\n",
    "    x_=np.sign(np.random.rand(B,K)-0.5)\n",
    "    y_=np.zeros([B,N])\n",
    "    w=np.random.randn(B,N)\n",
    "    Hy_=x_*0\n",
    "    H_ = np.zeros([B,N,K])\n",
    "    HH_= np.zeros([B,K,K])\n",
    "    SNR_= np.zeros([B])\n",
    "    x_ind = np.zeros([B,K,2])\n",
    "    for i in range(B):\n",
    "        for ii in range(K):\n",
    "            if x_[i][ii] == 1:\n",
    "                x_ind[i][ii][0] = 1\n",
    "            if x_[i][ii] == -1:\n",
    "                x_ind[i][ii][1] = 1  \n",
    "    for i in range(B):\n",
    "        #print i\n",
    "        SNR = np.random.uniform(low=snr_low,high=snr_high)\n",
    "        H=H_org\n",
    "        tmp_snr=(H.T.dot(H)).trace()/K\n",
    "        #H=H/np.sqrt(tmp_snr)*np.sqrt(SNR)\n",
    "        H_[i,:,:]=H\n",
    "        y_[i,:]=(H.dot(x_[i,:])+w[i,:]*np.sqrt(tmp_snr)/np.sqrt(SNR))\n",
    "        Hy_[i,:]=H.T.dot(y_[i,:])\n",
    "        HH_[i,:,:]=H.T.dot( H_[i,:,:])\n",
    "        SNR_[i] = SNR\n",
    "    return y_,H_,Hy_,HH_,x_,SNR_,x_ind,tmp_snr\n",
    "\n",
    "# Disable eager execution for compatibility with TensorFlow v1.x\n",
    "def piecewise_linear_soft_sign(x):\n",
    "    \"\"\"Custom soft sign activation.\"\"\"\n",
    "    t = tf.Variable(0.1, trainable=False)\n",
    "    y = -1 + tf.nn.relu(x + t) / (tf.abs(t) + 0.00001) - tf.nn.relu(x - t) / (tf.abs(t) + 0.00001)\n",
    "    return y\n",
    "\n",
    "def affine_layer(x, input_size, output_size, layer_num):\n",
    "    \"\"\"Affine layer for the neural network.\"\"\"\n",
    "    W = tf.Variable(tf.random_normal([input_size, output_size], stddev=0.01), name='W' + layer_num)\n",
    "    w = tf.Variable(tf.random_normal([1, output_size], stddev=0.01), name='w' + layer_num)\n",
    "    y = tf.matmul(x, W) + w\n",
    "    return y, W, w\n",
    "\n",
    "def relu_layer(x, input_size, output_size, layer_num):\n",
    "    \"\"\"ReLU layer.\"\"\"\n",
    "    y, W, w = affine_layer(x, input_size, output_size, layer_num)\n",
    "    y = tf.nn.relu(y)\n",
    "    return y, W, w\n",
    "\n",
    "def sign_layer(x, input_size, output_size, layer_num):\n",
    "    \"\"\"Sign activation layer.\"\"\"\n",
    "    y, W, w = affine_layer(x, input_size, output_size, layer_num)\n",
    "    return y, W, w\n",
    "# Zero Forcing Detector remains unchanged\n",
    "def zero_forcing_detector(H, y):\n",
    "    H_T_H = np.dot(H.T, H)\n",
    "    H_T_H_inv = np.linalg.inv(H_T_H)\n",
    "    H_T_y = np.dot(H.T, y)\n",
    "    x_hat = np.dot(H_T_H_inv, H_T_y)\n",
    "    return x_hat\n",
    "\n",
    "# MMSE Detector remains unchanged\n",
    "def mmse_detector(H, y, sigma2):\n",
    "    Nt = H.shape[1]\n",
    "    H_T_H = np.dot(H.T, H)\n",
    "    I = np.eye(Nt)\n",
    "    H_T_H_inv = np.linalg.inv(H_T_H + sigma2 * I)\n",
    "    H_T_y = np.dot(H.T, y)\n",
    "    x_hat_mmse = np.dot(H_T_H_inv, H_T_y)\n",
    "    return x_hat_mmse\n",
    "\n",
    "# Conjugate Gradient Correct function remains unchanged\n",
    "def conjugate_gradient_correct(A, B, s, max_iterations, tolerance,sigma_square=0):\n",
    "    A=A+sigma_square\n",
    "    n = len(B)\n",
    "    s_exp = np.zeros((n, max_iterations + 1))\n",
    "    s_exp[:, 0] = s\n",
    "    rk = B - A @ s\n",
    "    dk = rk\n",
    "    iter_max = 0\n",
    "\n",
    "    for iter in range(max_iterations):\n",
    "        alpha = np.dot(rk, rk) / np.dot(dk, A @ dk)\n",
    "        s = s + alpha * dk\n",
    "        rk_new = rk - alpha * A @ dk\n",
    "        iter_max += 1\n",
    "\n",
    "        if np.linalg.norm(rk_new) < tolerance:\n",
    "            s_exp[:, iter + 1] = s\n",
    "            s_exp = s_exp[:, :iter + 2]\n",
    "            break\n",
    "\n",
    "        beta = np.dot(rk_new, rk_new) / np.dot(rk, rk)\n",
    "        dk = rk_new + beta * dk\n",
    "        rk = rk_new\n",
    "        s_exp[:, iter + 1] = s\n",
    "\n",
    "    if iter == max_iterations - 1:\n",
    "        s_exp = s_exp[:, :max_iterations + 1]\n",
    "\n",
    "    return s_exp, iter_max\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
